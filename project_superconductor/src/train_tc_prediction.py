"""
è¶…å¯¼ææ–™ä¸´ç•Œæ¸©åº¦é¢„æµ‹æ¨¡å‹è®­ç»ƒæ¨¡å—
å®ç°ç‰©ç†çº¦æŸæŸå¤±ã€äº¤å‰éªŒè¯å’Œå®Œæ•´è®­ç»ƒæµç¨‹
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.model_selection import KFold
import os
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import sys

# ==================== ç³»ç»Ÿè·¯å¾„é…ç½® ====================
# ç¡®ä¿é¡¹ç›®æ¨¡å—æ­£ç¡®å¯¼å…¥ï¼Œæ”¯æŒå¤šç§è¿è¡Œç¯å¢ƒ
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.append(current_dir)
sys.path.append(parent_dir)

# åŠ¨æ€å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—ï¼ˆå…¼å®¹ä¸åŒç›®å½•ç»“æ„ï¼‰
try:
    from data_processor import SuperconDataProcessor
    from model import TcPredictor, TcPredictorAdvanced
except ImportError:
    from src.data_processor import SuperconDataProcessor
    from src.model import TcPredictor, TcPredictorAdvanced


# ==================== ç‰©ç†çº¦æŸæŸå¤±å‡½æ•°ç±» ====================
class PhysicsConstrainedLoss(nn.Module):
    """
    ç‰©ç†çº¦æŸå¢å¼ºæŸå¤±å‡½æ•°
    åœ¨æ ‡å‡†å›å½’æŸå¤±åŸºç¡€ä¸Šæ·»åŠ è¶…å¯¼ç‰©ç†å…ˆéªŒçº¦æŸ
    """
    
    def __init__(self, base_loss='mse', constraint_weights=None):
        """
        åˆå§‹åŒ–ç‰©ç†çº¦æŸæŸå¤±
        
        å‚æ•°:
            base_loss (str): åŸºç¡€æŸå¤±å‡½æ•°ç±»å‹ï¼Œå¯é€‰'mse'æˆ–'l1'
            constraint_weights (dict): å„çº¦æŸé¡¹çš„æƒé‡é…ç½®
        """
        super().__init__()
        # åŸºç¡€æŸå¤±å‡½æ•°é€‰æ‹©
        self.base_loss_fn = nn.MSELoss() if base_loss == 'mse' else nn.L1Loss()
        
        # é»˜è®¤çº¦æŸæƒé‡ï¼ˆéè´Ÿçº¦æŸå’Œä¸Šç•Œçº¦æŸï¼‰
        self.weights = {'non_negative': 0.1, 'upper_bound': 0.1}
        
        # æ›´æ–°ç”¨æˆ·è‡ªå®šä¹‰æƒé‡é…ç½®
        if constraint_weights: 
            self.weights.update(constraint_weights)
    
    def forward(self, predictions, targets, features=None):
        """
        å‰å‘ä¼ æ’­è®¡ç®—æ€»æŸå¤±
        
        å‚æ•°:
            predictions (Tensor): æ¨¡å‹é¢„æµ‹å€¼
            targets (Tensor): çœŸå®æ ‡ç­¾å€¼
            features (Tensor): è¾“å…¥ç‰¹å¾ï¼ˆå¯é€‰ï¼Œç”¨äºæ‰©å±•çº¦æŸï¼‰
            
        è¿”å›:
            total_loss (Tensor): æ€»æŸå¤±å€¼
            loss_components (dict): å„æŸå¤±åˆ†é¡¹ç»Ÿè®¡
        """
        # 1. è®¡ç®—åŸºç¡€å›å½’æŸå¤±ï¼ˆå‡æ–¹è¯¯å·®ï¼‰
        base_loss = self.base_loss_fn(predictions, targets)
        
        # 2. çº¦æŸ1: éè´Ÿæ€§çº¦æŸ (Tc â‰¥ 0 K)
        neg_loss = torch.mean(torch.relu(-predictions) ** 2) if self.weights['non_negative'] > 0 else 0.0
        
        # 3. çº¦æŸ2: åˆç†ä¸Šç•Œçº¦æŸ (Tc < 350 Kï¼ŒåŸºäºå·²çŸ¥è¶…å¯¼ææ–™æ¸©åº¦ä¸Šé™)
        high_loss = torch.mean(torch.relu(predictions - 350.0) ** 2) if self.weights['upper_bound'] > 0 else 0.0
        
        # 4. åŠ æƒç»„åˆå„æŸå¤±é¡¹
        total_loss = base_loss + self.weights['non_negative'] * neg_loss + self.weights['upper_bound'] * high_loss
        
        # è¿”å›æ€»æŸå¤±åŠå„åˆ†é¡¹ç»Ÿè®¡ï¼ˆç”¨äºç›‘æ§ï¼‰
        return total_loss, {'base': base_loss.item(), 'neg': neg_loss, 'high': high_loss}


# ==================== äº¤å‰éªŒè¯æµç¨‹å‡½æ•° ====================
def run_cross_validation(X, y, model_class, k_folds=5, epochs=50, 
                         batch_size=64, learning_rate=0.001, device='cpu', 
                         use_physics_constraints=False):
    """
    æ‰§è¡ŒKæŠ˜äº¤å‰éªŒè¯è¯„ä¼°æ¨¡å‹æ€§èƒ½
    
    å‚æ•°:
        X (np.ndarray): ç‰¹å¾çŸ©é˜µ
        y (np.ndarray): æ ‡ç­¾å‘é‡
        model_class: æ¨¡å‹ç±»æ„é€ å‡½æ•°
        k_folds (int): äº¤å‰éªŒè¯æŠ˜æ•°
        epochs (int): æ¯æŠ˜è®­ç»ƒè½®æ¬¡
        batch_size (int): æ‰¹é‡å¤§å°
        learning_rate (float): å­¦ä¹ ç‡
        device (str): è®¡ç®—è®¾å¤‡
        use_physics_constraints (bool): æ˜¯å¦å¯ç”¨ç‰©ç†çº¦æŸæŸå¤±
        
    è¿”å›:
        avg_r2 (float): å¹³å‡RÂ²åˆ†æ•°
        avg_rmse (float): å¹³å‡RMSEè¯¯å·®
        model_history (list): å„æŠ˜è®­ç»ƒå†å²ï¼ˆæš‚æœªè¿”å›ï¼‰
    """
    print(f"\nğŸ”„ å¼€å§‹{k_folds}æŠ˜äº¤å‰éªŒè¯...")
    
    # å¼ºåˆ¶ä½¿ç”¨CPUè®¾å¤‡ï¼ˆè§£å†³RTX 5070ç­‰æ–°ç¡¬ä»¶çš„CUDAå…¼å®¹æ€§é—®é¢˜ï¼‰
    device = torch.device("cpu")
    
    # åˆå§‹åŒ–KæŠ˜åˆ†å‰²å™¨
    kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)
    fold_results = []  # å­˜å‚¨å„æŠ˜è¯„ä¼°ç»“æœ
    
    # é€æŠ˜è®­ç»ƒå’Œè¯„ä¼°
    for fold, (train_ids, val_ids) in enumerate(kfold.split(X)):
        # æ•°æ®åˆ†å‰²ä¸è®¾å¤‡è½¬æ¢
        X_train = torch.FloatTensor(X[train_ids]).to(device)
        y_train = torch.FloatTensor(y[train_ids]).to(device)
        X_val = torch.FloatTensor(X[val_ids]).to(device)
        y_val = torch.FloatTensor(y[val_ids]).to(device)
        
        # æ¨¡å‹åˆå§‹åŒ–
        model = model_class(input_size=X.shape[1], dropout_rate=0.2).to(device)
        optimizer = optim.Adam(model.parameters(), lr=learning_rate)
        
        # æŸå¤±å‡½æ•°é€‰æ‹©ï¼ˆç‰©ç†çº¦æŸæˆ–æ ‡å‡†æŸå¤±ï¼‰
        criterion = PhysicsConstrainedLoss() if use_physics_constraints else nn.MSELoss()
        
        # æ¨¡å‹è®­ç»ƒé˜¶æ®µ
        model.train()
        dataset = TensorDataset(X_train, y_train)
        loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
        
        # ç®€åŒ–çš„è®­ç»ƒå¾ªç¯ï¼ˆäº¤å‰éªŒè¯è½®æ¬¡è¾ƒå°‘ï¼‰
        for _ in range(epochs):
            for bx, by in loader:
                optimizer.zero_grad()
                out = model(bx)
                # ç‰©ç†çº¦æŸæŸå¤±è¿”å›å…ƒç»„ï¼Œæ ‡å‡†æŸå¤±è¿”å›æ ‡é‡
                loss = criterion(out, by)[0] if use_physics_constraints else criterion(out, by)
                loss.backward()
                optimizer.step()
        
        # æ¨¡å‹è¯„ä¼°é˜¶æ®µ
        model.eval()
        with torch.no_grad():
            preds = model(X_val).cpu().numpy().flatten()
            targets = y_val.cpu().numpy().flatten()
            
            # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
            r2 = r2_score(targets, preds)
            rmse = np.sqrt(mean_squared_error(targets, preds))
            neg_count = np.sum(preds < 0)  # ç»Ÿè®¡ä¸åˆç†è´Ÿé¢„æµ‹
            
            fold_results.append({'r2': r2, 'rmse': rmse, 'neg': neg_count})
            
        # è¾“å‡ºæœ¬æŠ˜ç»“æœ
        print(f"   æŠ˜{fold+1}: RÂ²={r2:.4f}, RMSE={rmse:.4f}, è´Ÿå€¼æ•°={neg_count}")
    
    # è®¡ç®—äº¤å‰éªŒè¯å¹³å‡æ€§èƒ½
    avg_r2 = np.mean([r['r2'] for r in fold_results])
    avg_rmse = np.mean([r['rmse'] for r in fold_results])
    print(f"âœ… äº¤å‰éªŒè¯å¹³å‡: RÂ²={avg_r2:.4f}, RMSE={avg_rmse:.4f}")
    
    return avg_r2, avg_rmse, None


# ==================== ä¸»è®­ç»ƒæµç¨‹å‡½æ•° ====================
def train_model(data_path, model_class=TcPredictorAdvanced, epochs=500, batch_size=64, 
                learning_rate=0.001, do_cv=True, use_advanced_features=True, 
                use_physics_constraints=False, constraint_weights=None):
    """
    å®Œæ•´æ¨¡å‹è®­ç»ƒæµç¨‹ï¼ŒåŒ…æ‹¬æ•°æ®å‡†å¤‡ã€äº¤å‰éªŒè¯å’Œæœ€ç»ˆè®­ç»ƒ
    
    å‚æ•°:
        data_path (str): æ•°æ®æ–‡ä»¶è·¯å¾„
        model_class: æ¨¡å‹ç±»ï¼ˆé»˜è®¤ä½¿ç”¨é«˜çº§æ¨¡å‹ï¼‰
        epochs (int): æ€»è®­ç»ƒè½®æ¬¡
        batch_size (int): è®­ç»ƒæ‰¹é‡å¤§å°
        learning_rate (float): åˆå§‹å­¦ä¹ ç‡
        do_cv (bool): æ˜¯å¦æ‰§è¡Œäº¤å‰éªŒè¯
        use_advanced_features (bool): æ˜¯å¦ä½¿ç”¨é«˜çº§ç‰©ç†ç‰¹å¾
        use_physics_constraints (bool): æ˜¯å¦å¯ç”¨ç‰©ç†çº¦æŸ
        constraint_weights (dict): ç‰©ç†çº¦æŸæƒé‡é…ç½®
        
    è¿”å›:
        model (nn.Module): è®­ç»ƒå®Œæˆçš„æœ€ä½³æ¨¡å‹
        final_r2 (float): æœ€ç»ˆRÂ²åˆ†æ•°
        final_rmse (float): æœ€ç»ˆRMSEè¯¯å·®
    """
    print(f"--- è®­ç»ƒæµç¨‹å¯åŠ¨ ---")
    print(f"é…ç½®: é«˜çº§ç‰¹å¾={use_advanced_features}, ç‰©ç†æŸå¤±={use_physics_constraints}")
    
    # å¼ºåˆ¶ä½¿ç”¨CPUæ¨¡å¼ï¼ˆé¿å…RTX 5070ç­‰æ–°æ˜¾å¡çš„CUDAç‰ˆæœ¬å…¼å®¹æ€§é—®é¢˜ï¼‰
    device = torch.device("cpu")
    print(f"âš ï¸ å¼ºåˆ¶å¯ç”¨CPUæ¨¡å¼ï¼ˆå› RTX 5070 CUDAç‰ˆæœ¬ä¸å…¼å®¹ï¼‰")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")

    # 1. æ•°æ®é¢„å¤„ç†ä¸ç‰¹å¾å·¥ç¨‹
    processor = SuperconDataProcessor(use_advanced_features=use_advanced_features)
    X_train_np, X_test_np, y_train_np, y_test_np = processor.load_and_process_data(data_path)
    
    # 2. äº¤å‰éªŒè¯è¯„ä¼°ï¼ˆå¯é€‰ï¼‰
    if do_cv:
        run_cross_validation(X_train_np, y_train_np, model_class, k_folds=5, epochs=50, 
                             device=device, use_physics_constraints=use_physics_constraints)

    # 3. å…¨é‡æ•°æ®æœ€ç»ˆè®­ç»ƒ
    print("\nğŸš€ å¼€å§‹æœ€ç»ˆè®­ç»ƒ...")
    
    # æ•°æ®å¼ é‡è½¬æ¢
    X_train = torch.FloatTensor(X_train_np).to(device)
    y_train = torch.FloatTensor(y_train_np).to(device)
    X_test = torch.FloatTensor(X_test_np).to(device)
    y_test = torch.FloatTensor(y_test_np).to(device)
    
    # æ¨¡å‹åˆå§‹åŒ–
    model = model_class(input_size=X_train.shape[1], dropout_rate=0.2).to(device)
    
    # ä¼˜åŒ–å™¨é…ç½®ï¼ˆæ·»åŠ L2æ­£åˆ™åŒ–é˜²æ­¢è¿‡æ‹Ÿåˆï¼‰
    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆåŸºäºéªŒè¯æŸå¤±åŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡ï¼‰
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', 
                                                     factor=0.5, patience=15)
    
    # æŸå¤±å‡½æ•°é€‰æ‹©
    if use_physics_constraints:
        criterion = PhysicsConstrainedLoss(constraint_weights=constraint_weights)
    else:
        criterion = nn.MSELoss()
    
    # è®­ç»ƒçŠ¶æ€è·Ÿè¸ª
    best_val_loss = float('inf')
    train_losses, val_losses, r2_scores = [], [], []

    # æ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(TensorDataset(X_train, y_train), 
                              batch_size=batch_size, shuffle=True)
    
    # ä¸»è®­ç»ƒå¾ªç¯
    for epoch in range(epochs):
        model.train()
        ep_loss = 0
        
        # æ‰¹é‡è®­ç»ƒ
        for bx, by in train_loader:
            optimizer.zero_grad()
            out = model(bx)
            
            # æŸå¤±è®¡ç®—ï¼ˆåŒºåˆ†ç‰©ç†çº¦æŸå’Œæ ‡å‡†æŸå¤±ï¼‰
            if use_physics_constraints:
                loss, _ = criterion(out, by)
            else:
                loss = criterion(out, by)
            
            loss.backward()
            optimizer.step()
            ep_loss += loss.item()
            
        # éªŒè¯é˜¶æ®µ
        model.eval()
        with torch.no_grad():
            out_val = model(X_test)
            
            # éªŒè¯æŸå¤±è®¡ç®—
            if use_physics_constraints:
                val_loss_item = criterion(out_val, y_test)[0].item()
            else:
                val_loss_item = criterion(out_val, y_test).item()
            
            # éªŒè¯é›†é¢„æµ‹æ€§èƒ½
            val_preds = out_val.cpu().numpy().flatten()
            val_r2 = r2_score(y_test.cpu().numpy().flatten(), val_preds)
            
            # å­¦ä¹ ç‡è°ƒåº¦
            scheduler.step(val_loss_item)
            
            # æœ€ä½³æ¨¡å‹ä¿å­˜
            if val_loss_item < best_val_loss:
                best_val_loss = val_loss_item
                torch.save(model.state_dict(), 'best_model.pth')

        # è®°å½•è®­ç»ƒæŒ‡æ ‡
        train_losses.append(ep_loss / len(train_loader))
        val_losses.append(val_loss_item)
        r2_scores.append(val_r2)

        # å®šæœŸè¾“å‡ºè®­ç»ƒè¿›åº¦
        if (epoch+1) % 20 == 0:
            print(f"è½®æ¬¡ [{epoch+1}/{epochs}] éªŒè¯æŸå¤±: {val_loss_item:.4f}, RÂ²: {val_r2:.4f}")

    # ä¿å­˜è®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–å›¾è¡¨
    save_metrics_curves(train_losses, val_losses, r2_scores, epochs, use_physics_constraints)
    
    # åŠ è½½æœ€ä½³æ¨¡å‹è¿›è¡Œæœ€ç»ˆè¯„ä¼°
    model.load_state_dict(torch.load('best_model.pth'))
    model.eval()
    with torch.no_grad():
        final_preds = model(X_test).cpu().numpy().flatten()
        
        # ç”Ÿæˆé¢„æµ‹ç»“æœå¯è§†åŒ–
        create_prediction_plot(y_test.cpu().numpy().flatten(), 
                               final_preds, r2_scores[-1], use_physics_constraints)
        
    return model, r2_scores[-1], np.sqrt(best_val_loss)


def save_metrics_curves(train, val, r2, epochs, physics):
    """
    ä¿å­˜è®­ç»ƒè¿‡ç¨‹ä¸­çš„æŸå¤±å’ŒRÂ²æ›²çº¿å›¾
    
    å‚æ•°:
        train (list): è®­ç»ƒæŸå¤±å†å²
        val (list): éªŒè¯æŸå¤±å†å²
        r2 (list): RÂ²åˆ†æ•°å†å²
        epochs (int): è®­ç»ƒæ€»è½®æ¬¡
        physics (bool): æ˜¯å¦ä½¿ç”¨ç‰©ç†çº¦æŸï¼ˆç”¨äºæ–‡ä»¶ååŒºåˆ†ï¼‰
    """
    # æ ¹æ®æ˜¯å¦ä½¿ç”¨ç‰©ç†çº¦æŸç¡®å®šæ–‡ä»¶ååç¼€
    suffix = "_physics" if physics else "_baseline"
    
    # ç¡®ä¿å›¾è¡¨ç›®å½•å­˜åœ¨
    if not os.path.exists("./figures"): 
        os.makedirs("./figures")
    
    # åˆ›å»ºåŒé¢æ¿å›¾è¡¨
    plt.figure(figsize=(10, 5))
    
    # å·¦å›¾ï¼šæŸå¤±æ›²çº¿
    plt.subplot(1, 2, 1)
    plt.plot(train, label='è®­ç»ƒæŸå¤±')
    plt.plot(val, label='éªŒè¯æŸå¤±')
    plt.xlabel('è®­ç»ƒè½®æ¬¡')
    plt.ylabel('æŸå¤±å€¼')
    plt.title('æŸå¤±æ›²çº¿')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # å³å›¾ï¼šRÂ²åˆ†æ•°æ›²çº¿
    plt.subplot(1, 2, 2)
    plt.plot(r2, color='orange', label='RÂ²åˆ†æ•°')
    plt.xlabel('è®­ç»ƒè½®æ¬¡')
    plt.ylabel('RÂ²åˆ†æ•°')
    plt.title('é¢„æµ‹æ€§èƒ½æ›²çº¿')
    plt.ylim(0, 1)
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(f"./figures/training_curve{suffix}.pdf", dpi=300, bbox_inches='tight')
    plt.close()


def create_prediction_plot(true, pred, r2, physics):
    """
    ç”Ÿæˆé¢„æµ‹å€¼ä¸çœŸå®å€¼çš„æ•£ç‚¹å¯¹æ¯”å›¾
    
    å‚æ•°:
        true (np.ndarray): çœŸå®å€¼æ•°ç»„
        pred (np.ndarray): é¢„æµ‹å€¼æ•°ç»„
        r2 (float): RÂ²åˆ†æ•°
        physics (bool): æ˜¯å¦ä½¿ç”¨ç‰©ç†çº¦æŸï¼ˆç”¨äºæ–‡ä»¶ååŒºåˆ†ï¼‰
    """
    suffix = "_physics" if physics else "_baseline"
    
    plt.figure(figsize=(6, 6))
    
    # æ•£ç‚¹å›¾ï¼Œé¢œè‰²è¡¨ç¤ºé¢„æµ‹è¯¯å·®å¤§å°
    plt.scatter(true, pred, alpha=0.5, c=np.abs(true-pred), 
                cmap='viridis', s=20, edgecolors='black', linewidth=0.5)
    
    # ç†æƒ³é¢„æµ‹çº¿ï¼ˆy=xï¼‰
    plt.plot([min(true), max(true)], [min(true), max(true)], 'r--', linewidth=2)
    
    plt.title(f"é¢„æµ‹å€¼ vs çœŸå®å€¼ (RÂ²={r2:.3f})", fontsize=14)
    plt.xlabel("çœŸå® Tc (K)", fontsize=12)
    plt.ylabel("é¢„æµ‹ Tc (K)", fontsize=12)
    
    # æ·»åŠ é¢œè‰²æ¡è¡¨ç¤ºè¯¯å·®å¤§å°
    plt.colorbar(label='ç»å¯¹è¯¯å·® (K)')
    
    plt.grid(True, alpha=0.3)
    plt.savefig(f"./figures/predictions{suffix}.pdf", dpi=300, bbox_inches='tight')
    plt.close()